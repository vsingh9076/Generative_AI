{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo37u+k/2APbSxKbTe/fPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsingh9076/Generative_AI/blob/main/Data_Preparation/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CAr3fZ9nyDkI"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import math\n",
        "\n",
        "\n",
        "def get_token_size(document, model):\n",
        "    tokenizer=tiktoken.encoding_for_model(model)\n",
        "    return len(tokenizer.encode(document))\n",
        "\n",
        "def naive_chunker(document, chunk_size, model):\n",
        "    tokenizer=tiktoken.encoding_for_model(model)\n",
        "    document_tokens=tokenizer.encode(document)\n",
        "    document_size = len(document_tokens)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, document_size, chunk_size):\n",
        "        chunk = document_tokens[i:i + chunk_size]\n",
        "        chunks.append(tokenizer.decode(chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def auto_chunker(document, max_chunk_size, model):\n",
        "    tokenizer = tiktoken.encoding_for_model(model)\n",
        "    document_tokens = tokenizer.encode(document)\n",
        "    document_size = len(document_tokens)\n",
        "    # total chunk number\n",
        "    K = math.ceil(document_size / max_chunk_size)\n",
        "    # average integer chunk size\n",
        "    average_chunk_size = math.ceil(document_size / K)\n",
        "    # number of chunks with average_chunk_size - 1\n",
        "    shorter_chunk_number = K * average_chunk_size - document_size\n",
        "    # number of chunks with average_chunk_size\n",
        "    standard_chunk_number = K - shorter_chunk_number\n",
        "\n",
        "    chunks = []\n",
        "    chunk_start = 0\n",
        "    for i in range(0, K):\n",
        "        if i < standard_chunk_number:\n",
        "            chunk_end = chunk_start + average_chunk_size\n",
        "        else:\n",
        "            chunk_end = chunk_start + average_chunk_size - 1\n",
        "        chunk = document_tokens[chunk_start:chunk_end]\n",
        "        chunks.append(tokenizer.decode(chunk))\n",
        "        chunk_start = chunk_end\n",
        "\n",
        "    assert chunk_start == document_size\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL='gpt-3.5-turbo'\n",
        "\n",
        "def comparison(test_doc, chunk_size):\n",
        "    print('document_size:', get_token_size(test_doc, MODEL))\n",
        "    print('max_chunk_size:', chunk_size)\n",
        "    naive_chunk_list=naive_chunker(test_doc, chunk_size, MODEL)\n",
        "    print('Naive chunking size list: ', [get_token_size(chunk, MODEL) for chunk in naive_chunk_list])\n",
        "    auto_chunk_list=auto_chunker(test_doc, chunk_size, MODEL)\n",
        "    print('Auto chunking size list: ', [get_token_size(chunk, MODEL) for chunk in auto_chunk_list])\n",
        "    print('----------------------------------------')\n",
        "\n",
        "\n",
        "CHUNK_SIZE=5\n",
        "single_token_text=\"abcd\"\n",
        "\n",
        "for i in range(10, 16):\n",
        "    test_doc=single_token_text*i\n",
        "    print('document:',test_doc)\n",
        "    comparison(test_doc, chunk_size=CHUNK_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwuOoBOHyNE8",
        "outputId": "fedb2c89-d86c-4ddf-a176-e1677ebb6228"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 10\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5]\n",
            "Auto chunking size list:  [5, 5]\n",
            "----------------------------------------\n",
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 11\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5, 1]\n",
            "Auto chunking size list:  [4, 4, 3]\n",
            "----------------------------------------\n",
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 12\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5, 2]\n",
            "Auto chunking size list:  [4, 4, 4]\n",
            "----------------------------------------\n",
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 13\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5, 3]\n",
            "Auto chunking size list:  [5, 4, 4]\n",
            "----------------------------------------\n",
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 14\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5, 4]\n",
            "Auto chunking size list:  [5, 5, 4]\n",
            "----------------------------------------\n",
            "document: abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcd\n",
            "document_size: 15\n",
            "max_chunk_size: 5\n",
            "Naive chunking size list:  [5, 5, 5]\n",
            "Auto chunking size list:  [5, 5, 5]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsingh9076/Generative_AI/blob/main/Semantic_Search/ReRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Credit : [James Briggs](https://www.youtube.com/watch?v=Uh9bYiVrW_s&t=6s)"
      ],
      "metadata": {
        "id": "eG5GRJvAy-yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oM0rp3czy72H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4-9hR3iJh1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUj-DTsHiJh3"
      },
      "source": [
        "# Rerankers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bae1gf6xiJh3"
      },
      "source": [
        "Rerankers have been a common component of retrieval pipelines for many years. They allow us to add a final \"reranking\" step to our retrieval pipelines — like with **R**etrieval **A**ugmented **G**eneration (RAG) — that can be used to dramatically optimize our retrieval pipelines and improve their accuracy.\n",
        "\n",
        "In the example notebook we'll learn how to create retrieval pipelines with reranking using the [Cohere reranking model](https://txt.cohere.com/rerank/) (which is available for free).\n",
        "\n",
        "To begin, we setup our prerequisite libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "thtg9njP4bOh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    datasets==2.14.5 \\\n",
        "    openai==0.28.1 \\\n",
        "    pinecone-client==2.2.4 \\\n",
        "    cohere==4.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddl1qsEwiJh4"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY3OglQm4bOj"
      },
      "source": [
        "We start by downloading a dataset that we will encode and store. The dataset [`jamescalam/ai-arxiv-chunked`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked) contains scraped data from many popular ArXiv papers centred around LLMs. Including papers from Llama 2, GPTQ, and the GPT-4 technical paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQAVgquj4bOk",
        "outputId": "684b787c-9c66-4f3d-8abc-49f2f2187293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn1J-7cDiJh5"
      },
      "source": [
        "We have 41.5K chunks, where each chunk is roughly the length of 1-2 paragraphs in length. Here is an example of a single record:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQg8wiUQ4bOk",
        "outputId": "b59eac23-bcf4-4157-f2fb-d37e248dd23e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doi': '1910.01108',\n",
              " 'chunk-id': '0',\n",
              " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
              " 'id': '1910.01108',\n",
              " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
              " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
              " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
              " 'authors': ['Victor Sanh',\n",
              "  'Lysandre Debut',\n",
              "  'Julien Chaumond',\n",
              "  'Thomas Wolf'],\n",
              " 'categories': ['cs.CL'],\n",
              " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'published': '20191002',\n",
              " 'updated': '20200301',\n",
              " 'references': [{'id': '1910.01108'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFtJiIz4bOk"
      },
      "source": [
        "Format the data into the format we need, this will contain `id`, `text` (which we will embed), and `metadata`. For this use-case we don't need metadata but it can be useful to include so that if needed in the future we can make use of metadata filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-svyAMw4bOl",
        "outputId": "ab336af0-fd54-43fe-e1e5-329379a09d8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'text', 'metadata'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data = data.map(lambda x: {\n",
        "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
        "    \"text\": x[\"chunk\"],\n",
        "    \"metadata\": {\n",
        "        \"title\": x[\"title\"],\n",
        "        \"url\": x[\"source\"],\n",
        "        \"primary_category\": x[\"primary_category\"],\n",
        "        \"published\": x[\"published\"],\n",
        "        \"updated\": x[\"updated\"],\n",
        "        \"text\": x[\"chunk\"],\n",
        "    }\n",
        "})\n",
        "# drop uneeded columns\n",
        "data = data.remove_columns([\n",
        "    \"title\", \"summary\", \"source\",\n",
        "    \"authors\", \"categories\", \"comment\",\n",
        "    \"journal_ref\", \"primary_category\",\n",
        "    \"published\", \"updated\", \"references\",\n",
        "    \"doi\", \"chunk-id\",\n",
        "    \"chunk\"\n",
        "])\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYzwm_q_4bOl"
      },
      "source": [
        "We need to define an embedding model to create our embedding vectors for retrieval, for that we will be using OpenAI's text-embedding-ada-002. There is some cost associated with this model, so be aware of that (costs for running this notebook are <$1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVjJ6gGd4bOl",
        "outputId": "6b446f29-ec4a-41f9-b944-2cddfaba95c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT4Tokenizer'. \n",
            "The class this function is called from is 'GPT2TokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')\n",
        "\n",
        "\n",
        "def get_embedding(text):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndAuMyYC4bOm"
      },
      "source": [
        "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key and environment variable are found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThmEhCcI4bOm",
        "outputId": "1feb7bc2-bcc8-4609-a0a7-8b210aa0746c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gcp-starter\n"
          ]
        }
      ],
      "source": [
        "import pinecone\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('pinecone_api_key')\n",
        "# find your environment next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or input()\n",
        "\n",
        "pinecone.init(api_key=api_key, environment=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nu2KHWG4bOm"
      },
      "source": [
        "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4E9wrzx4bOm",
        "outputId": "3b70b496-7104-467e-f69c-053ecf794359"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"rerankers\"\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct'\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pinecone.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI76rcTi4bOm"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI's `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "tokenizer = SentenceTransformer('BAAI/bge-large-zh-v1.5')\n"
      ],
      "metadata": {
        "id": "BBDb5D-E4WxK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "a2xvoFt04bOn",
        "outputId": "623e598a-041d-4a29-91fd-54ba8cf75993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "b7371b7bd81c451dbba117b12c94ce4f",
            "538d3c539f2a4c179e76df8a40d7a598",
            "644ddf4b67a94da2800dd9aa316899ca",
            "c7fdca1969e542ce849f60f65a23624e",
            "7216f7108af447a2867119f1d170e4a2",
            "b6fe0b29202b42c281b6848442d88e36",
            "af3a8730fb3b40aa84df2c3e2cd08a20",
            "e892465a0c2a4f408fd4ffa9ceeba123",
            "1af5a074894a4953b554b8400dc53625",
            "00cb1ced799a43d5bc0b9691d898b465",
            "f873d5a1474740adad04a11611e47988"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/416 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7371b7bd81c451dbba117b12c94ce4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8b984806da5f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to create embeddings.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# get embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mto_upsert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# upsert to Pinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "def get_embedding(text):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   print(\"text\")\n",
        "   print(text)\n",
        "   return tokenizer.encode(text)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    passed = False\n",
        "    # find end of batch\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # create batch\n",
        "    batch = data[i:i_end]\n",
        "    # create embeddings (exponential backoff to avoid RateLimitError)\n",
        "    for j in range(1):  # max 5 retries\n",
        "        try:\n",
        "            # text = batch[\"text\"] # .replace(\"\\n\", \" \")\n",
        "            text = [s.replace('\\n', ' ') for s in batch[\"text\"]]\n",
        "            res = tokenizer.encode(text, normalize_embeddings=True)\n",
        "            #res = get_embedding(batch[\"text\"])\n",
        "            passed = True\n",
        "        except:\n",
        "            time.sleep(2**j)  # wait 2^j seconds before retrying\n",
        "            print(\"Retrying...\")\n",
        "    if not passed:\n",
        "        raise RuntimeError(\"Failed to create embeddings.\")\n",
        "    # get embeddings\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyFZKhUa4bOn"
      },
      "source": [
        "Now let's test retrieval _without_ Cohere's reranking model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pUo5EQK4bOn"
      },
      "outputs": [],
      "source": [
        "def get_docs(query: str, top_k: int):\n",
        "    # encode query\n",
        "    xq = get_embedding([query])[0]\n",
        "    # search pinecone index\n",
        "    res = index.query(xq, top_k=top_k, include_metadata=True)\n",
        "    # get doc text\n",
        "    docs = {x[\"metadata\"]['text']: i for i, x in enumerate(res[\"matches\"])}\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3FASr-04bOn",
        "outputId": "b6283220-a24e-43e4-a8c6-998283f081ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\n",
            "the likelihood that their ﬁnal answer is correct.\n",
            "RLHF has emerged as a powerful strategy for ﬁne-tuning Large Language Models, enabling signiﬁcant\n",
            "improvements in their performance (Christiano et al., 2017). The method, ﬁrst showcased by Stiennon et al.\n",
            "(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\n",
            "In this paradigm, models are ﬁne-tuned based on feedback from human users, thus iteratively aligning the\n",
            "models’ responses more closely with human expectations and preferences.\n",
            "Ouyang et al. (2022) demonstrates that a combination of instruction ﬁne-tuning and RLHF can help ﬁx\n",
            "issues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\n",
            "et al. (2022b) partially automates this ﬁne-tuning-plus-RLHF approach by replacing the human-labeled\n",
            "ﬁne-tuningdatawiththemodel’sownself-critiquesandrevisions,andbyreplacinghumanraterswitha\n",
            "---\n",
            "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
            "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
            "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
            "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
            "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
            "to control for the amount of RLHF training in the analysis of our experiments.\n",
            "3.2 Experiments\n",
            "3.2.1 Overview\n",
            "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
            "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
            "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
            "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
            "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
            "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
            "---\n",
            "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
            "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
            "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
            "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
            "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
            "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
            "region where RL reward remains linear inp\n",
            "KL.\n",
            "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
            "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
            "KLwith a geodesic length in the Fisher geometry.\n",
            "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
            "might hope to detect unexpected behaviors emerging during RL training.\n",
            "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
            "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
            "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "query = \"can you explain why we would want to do rlhf?\"\n",
        "docs = get_docs(query, top_k=25)\n",
        "print(\"\\n---\\n\".join(docs.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNB4b8sl4bOn"
      },
      "source": [
        "Good, but can we get better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoFX0vSs4bOn"
      },
      "source": [
        "## Reranking Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpHf66Om4bOn"
      },
      "source": [
        "We can easily get the responses we need when we include _many_ responses, but this doesn't work well with LLMs. The recall performance for LLMs [decreases as we add more into the context window](https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/) — we call this excessive filling of the context window _\"context stuffing\"_.\n",
        "\n",
        "Fortunately reranking offers us a solution that helps us find those records that may not be within the top-3 results, and pull them into a smaller set of results to be given to the LLM.\n",
        "\n",
        "We will use Cohere's rerank endpoint for this, to use it you will need a [Cohere API key](https://dashboard.cohere.com/api-keys). Once you have your key you use it to create authenticate your Cohere client like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfiZ-KUV4bOo",
        "outputId": "0ced99fc-95c7-4557-dab3-7a2c3bfbcf08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import cohere\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\") or getpass.getpass()\n",
        "# init client\n",
        "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxt_cNPI4bOo"
      },
      "source": [
        "Now we can rerank our results with `co.rerank`. Let's try it with our earlier results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ucNH2dIXKD"
      },
      "outputs": [],
      "source": [
        "rerank_docs = co.rerank(\n",
        "    query=query, documents=docs.keys(), top_n=25, model=\"rerank-english-v2.0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeW4znDwJJjj"
      },
      "source": [
        "This returns a list of `RerankResult` objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9auAxaTJEPU",
        "outputId": "f9a53361-3066-4934-8304-d93d5b9cd903"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cohere.responses.rerank.RerankResult"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(rerank_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMWXA4YbJf-U"
      },
      "source": [
        "We access the text content of the docs like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "4ukXGwJ4JQhh",
        "outputId": "3bdacb5d-9233-4021-c8a8-91070511e3c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\\nthe likelihood that their ﬁnal answer is correct.\\nRLHF has emerged as a powerful strategy for ﬁne-tuning Large Language Models, enabling signiﬁcant\\nimprovements in their performance (Christiano et al., 2017). The method, ﬁrst showcased by Stiennon et al.\\n(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\\nIn this paradigm, models are ﬁne-tuned based on feedback from human users, thus iteratively aligning the\\nmodels’ responses more closely with human expectations and preferences.\\nOuyang et al. (2022) demonstrates that a combination of instruction ﬁne-tuning and RLHF can help ﬁx\\nissues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\\net al. (2022b) partially automates this ﬁne-tuning-plus-RLHF approach by replacing the human-labeled\\nﬁne-tuningdatawiththemodel’sownself-critiquesandrevisions,andbyreplacinghumanraterswitha'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rerank_docs[0].document[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4e9yo88iJh7"
      },
      "source": [
        "The reordered results look like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lOqKxhIy4C",
        "outputId": "2313686c-f51e-40c2-d492-d4b7523f9306"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 23,\n",
              " 14,\n",
              " 3,\n",
              " 12,\n",
              " 6,\n",
              " 9,\n",
              " 8,\n",
              " 1,\n",
              " 17,\n",
              " 7,\n",
              " 21,\n",
              " 2,\n",
              " 16,\n",
              " 10,\n",
              " 20,\n",
              " 18,\n",
              " 22,\n",
              " 24,\n",
              " 13,\n",
              " 19,\n",
              " 4,\n",
              " 15,\n",
              " 11,\n",
              " 5]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[docs[doc.document[\"text\"]] for doc in rerank_docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKiUwIikMGU1"
      },
      "source": [
        "Let's write a function to allow us to more easily compare the original results vs. reranked results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfFFNLu2MLrt"
      },
      "outputs": [],
      "source": [
        "def compare(query: str, top_k: int, top_n: int):\n",
        "    # first get vec search results\n",
        "    docs = get_docs(query, top_k=top_k)\n",
        "    i2doc = {docs[doc]: doc for doc in docs.keys()}\n",
        "    # rerank\n",
        "    rerank_docs = co.rerank(\n",
        "        query=query, documents=docs.keys(), top_n=top_n, model=\"rerank-english-v2.0\"\n",
        "    )\n",
        "    original_docs = []\n",
        "    reranked_docs = []\n",
        "    # compare order change\n",
        "    for i, doc in enumerate(rerank_docs):\n",
        "        rerank_i = docs[doc.document[\"text\"]]\n",
        "        print(str(i)+\"\\t->\\t\"+str(rerank_i))\n",
        "        if i != rerank_i:\n",
        "            reranked_docs.append(f\"[{rerank_i}]\\n\"+doc.document[\"text\"])\n",
        "            original_docs.append(f\"[{i}]\\n\"+i2doc[i])\n",
        "    for orig, rerank in zip(original_docs, reranked_docs):\n",
        "        print(\"ORIGINAL:\\n\"+orig+\"\\n\\nRERANKED:\\n\"+rerank+\"\\n\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwJW3CqyiJh7"
      },
      "source": [
        "Beginning with our `\"can you explain why we would want to do rlhf?\"` query, let's take a look at the top-3 results with / without reranking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwcRIVX-Ng6N",
        "outputId": "6c595b79-76a8-40e2-eb9e-a64d9d366813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\t->\t0\n",
            "1\t->\t23\n",
            "2\t->\t14\n",
            "ORIGINAL:\n",
            "[1]\n",
            "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
            "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
            "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
            "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
            "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
            "to control for the amount of RLHF training in the analysis of our experiments.\n",
            "3.2 Experiments\n",
            "3.2.1 Overview\n",
            "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
            "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
            "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
            "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
            "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
            "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
            "\n",
            "RERANKED:\n",
            "[23]\n",
            "We have shown that it’s possible to use reinforcement learning from human feedback to train language models\n",
            "that act as helpful and harmless assistants. Our RLHF training also improves honesty, though we expect\n",
            "other techniques can do better still. As in other recent works associated with aligning large language models\n",
            "[Stiennon et al., 2020, Thoppilan et al., 2022, Ouyang et al., 2022, Nakano et al., 2021, Menick et al., 2022],\n",
            "RLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models\n",
            "up.\n",
            "Our alignment interventions actually enhance the capabilities of large models, and can easily be combined\n",
            "with training for specialized skills (such as coding or summarization) without any degradation in alignment\n",
            "or performance. Models with less than about 10B parameters behave differently, paying an ‘alignment tax’ on\n",
            "their capabilities. This provides an example where models near the state-of-the-art may have been necessary\n",
            "to derive the right lessons from alignment research.\n",
            "The overall picture we seem to ﬁnd – that large models can learn a wide variety of skills, including alignment, in a mutually compatible way – does not seem very surprising. Behaving in an aligned fashion is just\n",
            "another capability, and many works have shown that larger models are more capable [Kaplan et al., 2020,\n",
            "\n",
            "---\n",
            "\n",
            "ORIGINAL:\n",
            "[2]\n",
            "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
            "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
            "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
            "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
            "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
            "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
            "region where RL reward remains linear inp\n",
            "KL.\n",
            "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
            "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
            "KLwith a geodesic length in the Fisher geometry.\n",
            "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
            "might hope to detect unexpected behaviors emerging during RL training.\n",
            "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
            "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
            "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
            "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
            "\n",
            "RERANKED:\n",
            "[14]\n",
            "the model outputs safe responses, they are often more detailed than what the average annotator writes.\n",
            "Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
            "teachthemodelhowtowritemorenuancedresponses. ComprehensivetuningwithRLHFhastheadded\n",
            "beneﬁt that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).\n",
            "WeconductRLHFbyﬁrstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators\n",
            "writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto\n",
            "theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines. Wethenusethehuman\n",
            "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
            "sample from the model during the RLHF stage.\n",
            "BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,\n",
            "wherethe challengecomesfrom asmallnumber ofveryspeciﬁc cases. Weinvestigatetheimpact ofSafety\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compare(query, 25, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS9H0JGKiJh7"
      },
      "source": [
        "Both results from reranking provide many more reasons as to why we would want to use RLHF than the original records. Let's try another query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtqdxP9cQMUP",
        "outputId": "06f2d6e6-8024-4ba3-dd15-571063f2ac44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\t->\t0\n",
            "1\t->\t3\n",
            "2\t->\t17\n",
            "ORIGINAL:\n",
            "[1]\n",
            "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
            "beneﬁcial to experiment with the formation of a community of AI red teaming professionals that draws\n",
            "together individuals from different organizations and bac kgrounds, speciﬁcally focused on some subset\n",
            "of AI (versus AI in general) that is relatively well-deﬁned a nd relevant across multiple organizations.25\n",
            "A community of red teaming professionals could take actions such as publish best practices, collectively\n",
            "analyze particular case studies, organize workshops on eme rging issues, or advocate for policies that\n",
            "would enable red teaming to be more effective.\n",
            "Doing red teaming in a more collaborative fashion, as a commu nity of focused professionals across\n",
            "23Red teaming could be aimed at assessing various properties o f AI systems, though we focus on safety and security in this\n",
            "subsection given the expertise of the authors who contribut ed to it.\n",
            "24For an example of early efforts related to this, see Marshall et al., \"Threat Modeling AI /ML Systems and Dependencies\"\n",
            "[43]\n",
            "25In the context of language models, for example, 2019 saw a deg ree of communication and coordination across AI developers\n",
            "to assess the relative risks of different language understa nding and generation systems [10]. Adversarial machine learning,\n",
            "\n",
            "RERANKED:\n",
            "[3]\n",
            "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
            "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
            "respect to their safety and security claims, at least to the e xtent that effective red teaming practices exist\n",
            "and are demonstrably employed.\n",
            "As indicated by the number of cases in which AI systems cause o r threaten to cause harm, developers of an\n",
            "AI system often fail to anticipate the potential risks assoc iated with technical systems they develop. These\n",
            "risks include both inadvertent failures and deliberate mis use. Those not involved in the development\n",
            "of a particular system may be able to more easily adopt and pra ctice an attacker’s skillset. A growing\n",
            "number of industry labs have dedicated red teams, although b est practices for such efforts are generally\n",
            "in their early stages.24There is a need for experimentation both within and across or ganizations in order\n",
            "to move red teaming in AI forward, especially since few AI dev elopers have expertise in relevant areas\n",
            "such as threat modeling and adversarial machine learning [44].\n",
            "AI systems and infrastructure vary substantially in terms o f their properties and risks, making in-house\n",
            "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
            "\n",
            "---\n",
            "\n",
            "ORIGINAL:\n",
            "[2]\n",
            "more comprehensive way.\n",
            "We conducted a series of red teaming with various groups of internal employees, contract workers, and\n",
            "externalvendors. Theseteamsincludedover350people,includingdomainexpertsincybersecurity,election fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine\n",
            "learning, responsible AI, and creative writing. They also included individuals representative of a variety of\n",
            "socioeconomic, gender, ethnicity, and racial demographics.\n",
            "28\n",
            "Theredteamersprobedourmodelsacrossawiderangeofriskcategories(suchascriminalplanning,human\n",
            "traﬃcking, regulated or controlled substances, sexually explicit content, unqualiﬁed health or ﬁnancial\n",
            "advice, privacy violations, and more), as well as diﬀerent attack vectors (such as hypothetical questions,\n",
            "malformed/misspelledinputs,orextendeddialogues). Additionally,weconductedspeciﬁcteststodetermine\n",
            "the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and\n",
            "cyber); ﬁndingsonthesetopicsweremarginal andweremitigated. Nonetheless, wewill continueourred\n",
            "teaming eﬀorts in this front.\n",
            "\n",
            "RERANKED:\n",
            "[17]\n",
            "the training data [13], aiding in disinformation campaigns [12], generating extremist texts [37], spreading\n",
            "falsehoods [35], and more [9, 10, 18, 57, 22, 51]. As AI systems improve, the scope of possible harms seems\n",
            "likely to grow [22]. Many strategies have been developed to address some of these harms (e.g., [58, 4, 48,\n",
            "36, 34, 19, 60]). One potentially useful tool for addressing harm is red teaming—using manual or automated\n",
            "methods to adversarially probe a language model for harmful outputs, and then updating the model to avoid\n",
            "such outputs [42, 20, 3, 11]. In this paper, we describe our early efforts to implement manual red teaming to\n",
            "both make models safer and measure the safety of our models. The models trained with red team data were\n",
            "described in [4], so here we focus on describing our red team results and techniques in detail in the hope that\n",
            "others may beneﬁt from and improve on them.\n",
            "\u0003Correspondence to: {deep, liane, jackson, jared, jack}@anthropic.com\n",
            "Authors above the line break are core contributors. Author contributions are listed in §A.1.arXiv:2209.07858v2  [cs.CL]  22 Nov 2022\n",
            "2.7B 13B 52B\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "compare(\"what is red teaming?\", top_k=25, top_n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSUB3vfyiJh_"
      },
      "source": [
        "Again, the results provide more relevant responses when using reranking rather than the original search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8OfJFwq4bOo"
      },
      "source": [
        "Don't forget to delete your index when you're done to save resources!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQiU0IDl4bOo"
      },
      "outputs": [],
      "source": [
        "pinecone.delete(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gThAy0k4bOo"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7371b7bd81c451dbba117b12c94ce4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_538d3c539f2a4c179e76df8a40d7a598",
              "IPY_MODEL_644ddf4b67a94da2800dd9aa316899ca",
              "IPY_MODEL_c7fdca1969e542ce849f60f65a23624e"
            ],
            "layout": "IPY_MODEL_7216f7108af447a2867119f1d170e4a2"
          }
        },
        "538d3c539f2a4c179e76df8a40d7a598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6fe0b29202b42c281b6848442d88e36",
            "placeholder": "​",
            "style": "IPY_MODEL_af3a8730fb3b40aa84df2c3e2cd08a20",
            "value": "  0%"
          }
        },
        "644ddf4b67a94da2800dd9aa316899ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e892465a0c2a4f408fd4ffa9ceeba123",
            "max": 416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1af5a074894a4953b554b8400dc53625",
            "value": 0
          }
        },
        "c7fdca1969e542ce849f60f65a23624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00cb1ced799a43d5bc0b9691d898b465",
            "placeholder": "​",
            "style": "IPY_MODEL_f873d5a1474740adad04a11611e47988",
            "value": " 0/416 [06:42&lt;?, ?it/s]"
          }
        },
        "7216f7108af447a2867119f1d170e4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fe0b29202b42c281b6848442d88e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af3a8730fb3b40aa84df2c3e2cd08a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e892465a0c2a4f408fd4ffa9ceeba123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1af5a074894a4953b554b8400dc53625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00cb1ced799a43d5bc0b9691d898b465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f873d5a1474740adad04a11611e47988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}